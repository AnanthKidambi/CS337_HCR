# -*- coding: utf-8 -*-
"""CS337_proj.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1CrDG3nt7Ji7wQJm3QROdwCmDjQBXfaa6
"""

import torch
import torchvision.models  as models
import torchvision.transforms as transforms
from torchvision.io import read_image
from tqdm import tqdm

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

weights = models.VGG19_Weights.DEFAULT
model = models.vgg19(weights=weights, progress=True)
model = model.to(device)
model.eval()

print("Device being used:", device)

preprocess = weights.transforms(antialias=True)

img = read_image('input/tubingen.jpg')
img = img.to(device)

layer_names = [f"relu{i}_1" for i in range(1, 6)]
layer_nums = [2, 7, 12, 21, 30] # taken from https://www.mathworks.com/help/deeplearning/ref/vgg19.html

truncated_models = [torch.nn.Sequential(*list(model.features.children())[:layer_num]) for layer_num in layer_nums]

for i in layer_nums:
    print(i)
    assert model.features[i-1]._get_name() == "ReLU"

actual_outputs = []
for truncated_model in truncated_models:
    actual_outputs.append(truncated_model(preprocess(img).unsqueeze(0)))

pre_means = [0.485, 0.456, 0.406]
pre_stds = [0.229, 0.224, 0.225]

for i, truncated_model in enumerate(truncated_models):
    # create a random gaussian noise image
    # hyperparams:
    # conv1_2 : epochs-500, lr-0.05
    # conv2_2 : epochs-500, lr-0.05
    # conv3_2 : epochs-500, lr-0.05
    noise_img = torch.ones([3, 224, 224], device=device) - torch.rand([3, 224, 224], device = device)*0.5
    noise_img.requires_grad = True
    num_steps = 250
    optimizer = torch.optim.Adam([noise_img], lr=0.05)
    for j in tqdm(range(num_steps)):
        output = truncated_model(noise_img.unsqueeze(0))
        noise_img_ = noise_img.clone()
        noise_img_[0] = (noise_img_[0]*pre_stds[0]) + pre_means[0]
        noise_img_[1] = (noise_img_[1]*pre_stds[1]) + pre_means[1]
        noise_img_[2] = (noise_img_[2]*pre_stds[2]) + pre_means[2]

        dummy_loss = 0.5*((output - actual_outputs[i])**2).sum() + 40000*torch.relu(-noise_img_).sum()
        dummy_loss = 0.5*((output - actual_outputs[i])**2).sum()

        optimizer.zero_grad()
        dummy_loss.backward(retain_graph=True)

        optimizer.step()

    corr_img = noise_img.clone()
    corr_img[0] = (corr_img[0]*pre_stds[0]) + pre_means[0]
    corr_img[1] = (corr_img[1]*pre_stds[1]) + pre_means[1]
    corr_img[2] = (corr_img[2]*pre_stds[2]) + pre_means[2]

    print(corr_img[0].min())
    print(corr_img[1].min())
    print(corr_img[2].min())

    corr_img = transforms.ToPILImage()(corr_img.cpu())
    corr_img.save(f"output/{layer_names[i]}_corr.png")
